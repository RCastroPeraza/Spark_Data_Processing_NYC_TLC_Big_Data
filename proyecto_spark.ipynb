{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conexión con el cluster de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para leer los datos necesitas la ruta del archivo e indicar si tienen una cabecera o no, que en este caso si la tienen.\n",
    "datos_yellow = spark.read.csv(\"../data/yellow_tripdata_2019-01.csv\", header=True) #Es la ruta relativ a la ubicación relativa en el docker usado \n",
    "# Para visualizar los datos que leiste debes usar la función show e indicar el número de filas, en este caso 5.\n",
    "datos_yellow.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_fhv = spark.read.parquet(\"../data/tu_archivo.parquet\")\n",
    "datos_fhv.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1. Contar la cantidad total de viajes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modificación de codigo para la lectura de archivos parquet y csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa las funciones necesarias\n",
    "from operator import add\n",
    "\n",
    "# Ruta de entrada para los archivos CSV\n",
    "ruta_csv = \"../data/yellow_tripdata_2019-01.csv\"\n",
    "\n",
    "# Ruta de entrada para los archivos Parquet\n",
    "ruta_parquet = \"../data/*.parquet\"\n",
    "\n",
    "# Columnas que deseas seleccionar de los archivos CSV\n",
    "columnas_csv = [\"PULocationID\", \"passenger_count\"]  # Reemplaza con los nombres de tus columnas CSV\n",
    "\n",
    "# Columnas que deseas seleccionar de los archivos Parquet\n",
    "columnas_parquet = [\"pickup_datetime\", \"PULocationID\"]  # Reemplaza con los nombres de tus columnas Parquet\n",
    "\n",
    "# 1. Carga todos los archivos CSV y selecciona las columnas deseadas\n",
    "archivos_csv = spark.read.format(\"csv\").option(\"header\", \"true\").load(ruta_csv).select(columnas_csv)\n",
    "\n",
    "# 2. Carga todos los archivos Parquet y selecciona las columnas deseadas\n",
    "archivos_parquet = spark.read.format(\"parquet\").load(ruta_parquet).select(columnas_parquet)\n",
    "\n",
    "# 3. Convierte los DataFrames en RDD y realiza las transformaciones\n",
    "rdd_csv = archivos_csv.rdd \\\n",
    "    .filter(lambda linea: len(linea) > 1) \\\n",
    "    .map(lambda linea: (\"yellow\", 1) if len(linea) > 1 else (\"fhv\", 1))\n",
    "\n",
    "rdd_parquet = archivos_parquet.rdd \\\n",
    "    .map(lambda linea: (\"parquet\", 1))\n",
    "\n",
    "# 4. Une los RDD resultantes\n",
    "rdd_unido = rdd_csv.union(rdd_parquet)\n",
    "\n",
    "# 5. Aplica reduceByKey\n",
    "resultado = rdd_unido.reduceByKey(add)\n",
    "\n",
    "# Muestra el resultado\n",
    "resultado.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a usar la función count para contar la cantidad total de viajes de cada uno de los DataFrames.\n",
    "cantidad_yellow = datos_yellow.count()\n",
    "cantidad_fhv = datos_fhv.count()\n",
    "\n",
    "# Luego, usamos print para visualizar los resultados\n",
    "print(\"Cantidad de viajes de tipo yellow: \", cantidad_yellow)\n",
    "print(\"Cantidad de viajes de tipo fhv: \", cantidad_fhv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2. Contar las cantidad total de viajes por zona de recogida\n",
    "Para este ejercicio vas a contar la cantidad total de viajes para cada uno de las zonas de recogida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos_parquet.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si la columna 'mi_columna' contiene al menos un valor no nulo\n",
    "if archivos_parquet.filter(archivos_parquet.PULocationID.isNotNull()).count() > 0:\n",
    "    print(\"La columna 'mi_columna' contiene al menos un valor no nulo.\")\n",
    "else:\n",
    "    print(\"La columna 'mi_columna' está completamente compuesta por valores nulos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte el DataFrame 'archivos_csv' en un RDD 'rdd_csv'\n",
    "rdd_csv = archivos_csv.rdd\n",
    "\n",
    "# Ahora puedes aplicar tus transformaciones en 'rdd_csv'\n",
    "resultado_map_reduce_2 = rdd_csv.map(lambda linea: linea if isinstance(linea, str) else linea[0]).filter(lambda linea: isinstance(linea, str)).map(lambda linea: linea.split(\",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapea cada línea a una tupla con (PULocationID, 1) para contarlos\n",
    "conteo_por_PULocationID = rdd_csv.map(lambda linea: (linea[\"PULocationID\"], 1))\n",
    "\n",
    "# Realiza la reducción por clave para sumar o contar los valores\n",
    "resultado_map_reduce = conteo_por_PULocationID.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Muestra el resultado\n",
    "resultado_map_reduce.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena los resultados en orden descendente según la cantidad de registros\n",
    "resultado_ordenado_descendente = resultado_map_reduce.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Muestra el resultado ordenado descendente\n",
    "resultado_ordenado_descendente.collect()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
